\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{caption}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Breaking News Bandits}


\author{Achille Aknin \& Alban Pierre}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
Multi-armed bandit (MAB) is a very common problems, studied with many different approachs.
Besides, it is also derived in many ways, such as adversarial bandit, non-stationary bandit or distributed bandit.
Here we focus on a variation called
Breaking News multi-armed bandit, when one arm can have suddenly a high reward : this arm stays hot for a time before it comes back to normal.
In this report, we describe a model of such MABs and then expose some algorithms to maximize the rewards
on Breaking News MABs, before testing it on our model.
\end{abstract}

\section{Introduction}

Among differences from the classic multi armed bandit, the expectation can become
very high compared to normal rewards, so an algorithm should keep pulling the same
arm if it is hot. When no arms is hot, the algorithm should pull each arm frequently
in order to find a new breaking new. But during that time, the algorithm should
also maximize its reward, because each arm does not hove the same 'normal' reward.
We implemented several algorithms that have similar behaviors : in the first
section, we explain data generation and then in second part the widely used
Thompson Sampling and Upper Confidence Bound algorithms. Then we focus on an
exact but very slow algorithm (GM) in part three. In section four we explain an
algorithm that tries to detect whether we have a hot state based on maximum/minimum
previous rewards and in the next part based on the previous rewards variance.
Then in part six and seven we moved on an algorithm based on nearest neighbors.
Eventually in the eight section we provide more results for different problems.
\newline

Note : the code we used in our experiments is in matlab (compatible with octave) and can be found at the address : https://github.com/alban-pierre/projet-RL-Breaking\_news\_bandit


\section{Data generation}

To generate the data, we model each arm to have several states, and for each state the reward follows a Gaussian distribution of a mean and a variance specific to that state (and to that arm). In the following experiments we used the means and variance :
\begin{center}
\begin{tabular}{rccc}
	Arms : & 1st & 2nd & 3rd \\
	Mean of the 1st state : & 2.0 & 3.0 & 1.0 \\
	Variance of the 1st state : & 1.0 & 1.0 & 1.0 \\
	Mean of the 2nd state : & 70.0 & 50.0 & 80.0 \\
	Variance of the 2nd state : & 1.0 & 1.0 & 1.0 \\
\end{tabular}
\end{center}
For the transition probabilities : when one arm is in a hot state, this arm has a probability $p=0.1$ to come back to normal, the others stay at a normal state. When each arm is at the normal state, then we sample one arm randomly and it became hot with probability $p=0.03$. In such a model, there are at most one arm in the hot state. An example of a generated data with these parameters is shown in figure 1.
\newline

\begin{figure}[h]
	\begin{center}
		%\framebox[4.0in]{$\;$}
		%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
		\includegraphics[width=0.7\linewidth]{generated_data_one.png}
	\end{center}
	\caption{Example of generated data}
\end{figure}

For the last section, we also used a model where several arms can become hot at the same time, with approximately the same probabilities of transition.

\section{Upper Confidence Bound Algorithm}

In this section, we recall the widely used Upper Confidence Bound (UCB) Algorithm,
as described in [1],
that seeks to optimize the reward in the case of a regular MAB problem. The main
idea is to compute an expected mean for each arm $\hat\mu_a(t)$, upper bounding it
by a confidence on the mean being high for arms that haven't been drawn often. We
then draw the arm with the highest upper bound.
This way we make sure that every arm is used from time to time.
\begin{equation*}
	a^* = argmax_{a} \Big(\hat\mu_a(t) + \sqrt{\frac{log(t)}{2N_a(t)}}\Big)
\end{equation*}

\begin{algorithm}
	\caption{UCB Algorithm}
	\For{$i=1:T_{max}$}{
		Compute $a^* = argmax_{a} \Big(\hat\mu_a(t) + \sqrt{\frac{log(t)}{2N_a(t)}}\Big)$\;
		Draw arm $a^*$ and receive reward $r(t)$\;
		Update $\hat\mu_{a^*}(t+1) = \frac{N_{a^*}(t)\times \hat\mu_{a^*}(t) + r(t)}{N_{a^*}(t)+1}$\;
		Update $N_{a^*} = N_{a^*}+1$\;
	}
\end{algorithm}

This algorithm is meant to be used on a regular MAB, with reward functions that do not
change over time. In this report, we would like to take advantage of the fact that
we know the MAB follows the model described in last section. Most of the algorithms
we present on next section are based on the UCB algorithm, and adapted to fit our model
of Breaking News Bandit.

\section{Exact probabilities inference with Gaussian Mixture}

This first algorithm tries to be mathematically exact : we use a Gaussian mixture model using the Expectation-Maximisation algorithm to compute the mean of each state of each arm. Then given theses means (and variances) we compute the probability for each observed reward to come from one state. This gives for each state of each arm a sequence of probabilities. With theses sequence we compute the transition probabilities via a gradient descent. Thus, given the means and the transition probabilities, we can compute the expectation of each arm.
\newline

In practice, we add an exploration term to force the algorithm to explore, but even with that it does not lead to better results than Thompson-Sampling. The reason of that poor result is that when we compute the expectation of each arm, a little change in transition probabilities leads to a huge change in the expectation of an arm (at least if this arm was not seen for a few time).

Figure 3 show the exact expectation of an arm as a function of the time since the last draw. There are 2 curves for each arm because here each arm has 2 states (hot or not). Thus for one arm the upper curve is the expectation of that arm if the last time we draw it it was in the hot state, and the lower curve is the expectation of that arm if the last time we draw it it was in the normal state. The 2 curves converge because if we don't pull an arm for a long time, the probability to be hot is the same, whether it was hot or not the last time we draw that arm.

So in the case of the Gaussian Mixture algorithm, this is the plateau on the right that moves a lot due to approximations in transition probabilities, and then the outcome of the general algorithm is changed.

\begin{figure}[h]
	\begin{center}
		%\framebox[4.0in]{$\;$}
		%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
		\includegraphics[width=1.0\textwidth]{expectations.png}
	\end{center}
	\caption{Expectation of each arm as a function of time since the last draw}
\end{figure}

Besides, this plot gives insight of what algorithms should do : if an arm is in the hot state, the expectation is high so this algorithm should pull the same arm over and over until it is not hot anymore. When this hot arms become normal, the other arms were not pulled since a long time, so we are far on the right for these arms. Then we should chose the one that has the highest plateau. If this arm is not hot, then the expectation of this arm falls back to the beginning of the curve (on the bottom left). The expectation of the last arm not drawn is still on the plateau (so relatively high compared to other arms) and we should pull that arm.

Note that if all arms are normal, this plot gives an optimal sequence of draws : as we select the highest point, the points not selected step by one unit on the right, thus increasing. And we repeat taking the highest point. Thus the arm with the lowest normal reward is not selected offenly since it has to increase more to become the highest point.

\section{UCB\_Var}

In this section, we describe an algorithm based on the UCB algorithm and on an estimation of the maximal
value that an arm can reach when it is not hot, and use this estimation to detect when the
arm is hot. Our purpose is then to estimated from a sequence of samples on a given arm $a$ the range
of values we expect the next sample to be in. We will then exploit the mean $\hat\mu_a(t)$ and variance $\hat\sigma_a(t)$
estimated from the previous samples and consider that if an arm stays in its initial
state, the reward drawn $r$ should be in the range $r \in [\hat\mu_a(t)-\hat\sigma_a(t), \hat\mu_a(t)+\hat\sigma_a(t)]$.

To take into account the uncertainty on both the mean and the range we might have,
we will add a term decreasing with the number of sample rewards we have on the arm $a$:
\begin{equation}\label{UCB-var-bound}
	r \in \Big[\hat\mu_a(t)-\hat\sigma_a(t)-\sqrt{\frac{1}{2N_a(t)}}, \hat\mu_a(t)+\hat\sigma_a(t)+\sqrt{\frac{1}{2N_a(t)}}\Big]
\end{equation}

The algorithm is then as follows: we proceed as in the UCB algorithm, and if at
some time $t$ we draw a sample from an arm and the reward obtained $r(t)$ is higher than the upper bound
in Eq.~\ref{UCB-var-bound}, we then consider that this arm is in a hot state.
We temporarily forget the samples we've seen for this arm, and continue
with $r(t)$ as the only reward observed so far for arm $a$. This has the effect
of having a bigger mean $\hat\mu(t+1) = r(t)$ and having $N_a(t+1) = 1$, making this arm more suceptible
to be drawn next time. If at some time $t'$, we observe a reward $r(t')$ lower
than the lower bound in Eq.~\ref{UCB-var-bound}, we then consider that this arm has
left the hot state, and proceed with the samples observed before time $t$, having
once again a smaller mean $\hat\mu(t')$.

In practice, this algorithm gets quite stable and good results if we make sure,
with some adaptations, to not keep thinking that an arm is hot when it is actually
not. The main drawback of this algorithm is that we need to remember each
reward observed observed at along, and compute its variance, and we can't avoid
a linear operation at each step, so the algorithm has a complexity of $\mathcal{O}(N^2)$ where N
is the number of steps.

\section{UCB\_Max}

To avoid the quadratic complexity of the previous algorithm, we choose to not compute
the actual variance $\hat\sigma_a(t)$, but instead simply remember the maximal value
observed on each arm $M_a(t)$ (and for a hot state, the minimal value $m_a(t)$).
At each step, we now check whether $r(t) > M_a(t)$ (or whether $r(t) < m_a(t)$ for a hot state),
and if it is the case then we consider that arm $a$ entered the hot state (or left the hot state)
and we reinitialise $\hat\mu_a(t)$ (or take back the old value).

This algorithm does not have results as good as UCB\_Var, but it faster and only have
a linear complexity, since computing the mean $\hat\mu_a(t)$ and the max $M_a(t)$
can be done with a finite number of operations at each step.

\section{Nearest Neighbors Upper Confidence Bound Algorithms}

Here we try to estimate the expectations shown in figure 2. Unlike the Gaussian Mixture algorithm, we will use here a Monte-Carlo estimation. So the Nearest Neighbors algorithm starts by choosing each arm twice (initialization) and then from each arm, it embeds all previous rewards of this arm in the space (Last reward of the arm; Time since last draw; Reward observed). In practice the rewards are shifted and resized to fit in $[0,1]$ and the time since last draw in rather $exp(-time since last draw)$ so that a time of 1 is far from a time of 2 but a time of 56 is right next to time of 57.

Then we compute where is the current point in that space - along the two first dimensions - and we take the average along the third dimension (expected reward) of all points that are within a distance of $d=0.1$. If there are no points in that range we take the nearest point. Then as we did it for each arm we choose the arm that has obtained the maximum average plus the upper confidence bound variance.

In practice this algorithm performs at most like the UCB algorithm, but if we take the average over the $K$ nearest neighbors instead of the points within a $d$ range, then we get very good results.

\section{K-Nearest Neighbors Long Term Expectation}

Let's go back to the figure 2 : it show the best arm to draw given the last reward and the time since last draw. Unfortunately, this plot gives the greedy algorithm only for the next draw. Indeed we can think of an algorithm that draws a hot arm over and over, and then that draws a second arm even if the first arm is still hot, just because the expectation of that second arm is not $p(hot)*reward(hot) + p(normal)*reward(normal)$ but $p(ho	t)*reward(hot)*10 + p(normal)*reward(normal)$ if the algorithm knows that the second arms keeps being hot for at least 10 iterations.

This is the problem we tried to fix in this algorithm : first it tries to approximate the expectation just like KNN, but after a non-negligible number of draws, we add in the expectation the second reward obtained, and then a little while after, the third rewards obtained, etc. We also add a decay rate so that the first reward counts more than following rewards.

In practice it does not yield to an improvement because the change in probabilities is little so it does not change rewards significantly.


\section{Time of computation and results other different arms settings}

Here we show the results of each algorithms presented in last section on the multi
armed bandit settings presented in the Data generation section, while comparing them
with the results from UCB Algorithm and Thompson Sampling [2] :

\begin{figure}[h]
	\begin{center}
		%\framebox[4.0in]{$\;$}
		%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
		\includegraphics[width=1.0\textwidth]{all_10000it.png}
	\end{center}
	\caption{All algorithms for 10000 iterations (3 arms * 2 states, at most one hot arm)}
\end{figure}

\begin{figure}[h]
	\begin{minipage}[b]{.49\linewidth}
		\includegraphics[width=1.0\textwidth]{begin_1000it.png}
		\caption{1000 first iterations (3*2, one)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.49\linewidth}
		\includegraphics[width=1.0\textwidth]{last_1000it.png}
		\caption{1000 last iterations (3*2, one)}
	\end{minipage}
	\label{fig:f}
\end{figure}

Here are the computation time observed for 10000 iterations :
\begin{center}
	\begin{tabular}{rccccccc}
		Arms : & TS & UCB & GM & UCB\_KNN & KNN\_LONG & UCB\_MAX & UCB\_VAR \\
		Time (sec) : & 7.12 & 2.19 & 1000.5 & 25.39 & 35.85 & 3.06 & 5.37
	\end{tabular}
\end{center}

We can see that the performance of TS and UCB decrease over time, which is normal because they reduce exploration over time, and it is exploration that discovers hot states. The UCB\_KNN and KNN\_LONG increase their performance as theirs Monte-Carlo estimations are more and more precise. The UCB\_MAX and UCB\_VAR algorithms perform well in the beginning.

\clearpage
We also tried for data where several arms can become hot at the same time (with approximately the same transition probabilities, 5 runs each) :

\begin{figure}[h]
	\begin{center}
		%\framebox[4.0in]{$\;$}
		%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
		\includegraphics[width=1.0\textwidth]{all_m_10000it.png}
	\end{center}
	\caption{All algorithms for 10000 iterations (3 arms * 2 states, several hot arms)}
\end{figure}

\begin{figure}[h]
	\begin{minipage}[b]{.49\linewidth}
		\includegraphics[width=1.0\textwidth]{begin_m_1000it.png}
		\caption{1000 first iterations (3*2, several)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.49\linewidth}
		\includegraphics[width=1.0\textwidth]{last_m_1000it.png}
		\caption{1000 last iterations (3*2, several)}
	\end{minipage}
	\label{fig:f}
\end{figure}

We observe quite the same things except for KNN\_LONG : this algorithm sometimes tries different arms while they were on a hot arm with a comparatively low hot reward. The other algorithms keep hitting the same arm when it is hot, so they don't see the difference from the case where there are at most one hot arm.

\clearpage
Then we tried with 5 arms with 3 states each (with one hot arm at most, 10 runs each) : 

\begin{figure}[h]
	\begin{center}
		%\framebox[4.0in]{$\;$}
		%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
		\includegraphics[width=1.0\textwidth]{all_s_10000it.png}
	\end{center}
	\caption{All algorithms for 10000 iterations (5 arms * 3 states, at most one hot arm)}
\end{figure}

\begin{figure}[h]
	\begin{minipage}[b]{.49\linewidth}
		\includegraphics[width=1.0\textwidth]{begin_s_1000it.png}
		\caption{1000 first iterations (5*3, one)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.49\linewidth}
		\includegraphics[width=1.0\textwidth]{last_s_1000it.png}
		\caption{1000 last iterations (5*3, one)}
	\end{minipage}
	\label{fig:f}
\end{figure}

We observe that the UCB algorithm performs better, indeed as there are more arms, it explores more so it finds more hot states and high rewards.

\clearpage
Eventually we tried the same version with several hot arms (5 runs each) : 

\begin{figure}[h]
	\begin{center}
		%\framebox[4.0in]{$\;$}
		%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
		\includegraphics[width=1.0\textwidth]{all_ms_10000it.png}
	\end{center}
	\caption{All algorithms for 10000 iterations (5 arms * 3 states, several hot arms)}
\end{figure}

\begin{figure}[h]
	\begin{minipage}[b]{.49\linewidth}
		\includegraphics[width=1.0\textwidth]{begin_ms_1000it.png}
		\caption{1000 first iterations (5*3, several)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.49\linewidth}
		\includegraphics[width=1.0\textwidth]{last_ms_1000it.png}
		\caption{1000 last iterations (5*3, several)}
	\end{minipage}
	\label{fig:f}
\end{figure}

Here again the KNN\_LONG algorithm takes advantage for having a long term expectation calculation.

\clearpage
\section{Conclusion}

We have shown that for the special case of multi armed bandit problem where an arm can become hot and have a big reward, classical algorithms like Thompson Sampling or Upper Confidence Bound misses high rewards as they explore less and less across time. Indeed exploration finds more hot states, leading to a bigger reward.

To tackle this problem, we have tried to use the variance and the maximum of previous rewards of an arm to calculate when an arm is hot, then pulling always the hot arm. This lead to good results, but they don't improve over time. We have also used an algorithm that uses a Monte-Carlo approximation of expectation, it perform better in the end but it has a very long initialization phase.

In the case where many hot arms can become hot simultaneously, we have shown that an algorithm that pulls arms according to the maximum of its expectation over many following draws can perform better than greedy algorithms other the next draw. But it has still difficulties to get a significantly better result, and it could be interesting for further research to develop an algorithm that compute some sort of fixed point or else to get the best greedy policy.

\end{document}
